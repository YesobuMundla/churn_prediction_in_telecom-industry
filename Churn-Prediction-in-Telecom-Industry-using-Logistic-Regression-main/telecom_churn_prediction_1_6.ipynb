{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wdCGNjd1bgs7"
   },
   "source": [
    "# Business problem overview\n",
    "\n",
    "In the telecom industry, customers are able to choose from multiple service providers and actively switch from one operator to another. In this highly competitive market, the telecommunications industry experiences an average of 15-25% annual churn rate. Given the fact that it costs 5-10 times more to acquire a new customer than to retain an existing one, customer retention has now become even more important than customer acquisition.\n",
    "\n",
    " \n",
    "\n",
    "- For many incumbent operators, retaining high profitable customers is the number one business goal.\n",
    "\n",
    " \n",
    "\n",
    "- To reduce customer churn, telecom companies need to predict which customers are at high risk of churn.\n",
    "\n",
    " \n",
    "\n",
    "Here we will analyse customer-level data of a leading telecom firm, build predictive models to identify customers at high risk of churn and identify the main indicators of churn.\n",
    "\n",
    "\n",
    "Customers usually do not decide to switch to another competitor instantly, but rather over a period of time (this is especially applicable to high-value customers). In churn prediction, we assume that there are three phases of customer lifecycle :\n",
    "\n",
    "**The ‘good’ phase:** In this phase, the customer is happy with the service and behaves as usual.\n",
    "\n",
    "**The ‘action’ phase:** The customer experience starts to sore in this phase, for e.g. he/she gets a compelling offer from a  competitor, faces unjust charges, becomes unhappy with service quality etc. In this phase, the customer usually shows different behaviour than the ‘good’ months. Also, it is crucial to identify high-churn-risk customers in this phase, since some corrective actions can be taken at this point (such as matching the competitor’s offer/improving the service quality etc.)\n",
    "\n",
    "**The ‘churn’ phase:** In this phase, the customer is said to have churned. You define churn based on this phase. Also, it is important to note that at the time of prediction (i.e. the action months), this data is not available to you for prediction. Thus, after tagging churn as 1/0 based on this phase, you discard all data corresponding to this phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfJSn_ExdgiA"
   },
   "source": [
    "# Exporting required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "eDMkkY9Df98B"
   },
   "outputs": [],
   "source": [
    "# Installing external packages\n",
    "#!pip install missingno\n",
    "#!pip install imbalanced-learn\n",
    "#!pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fy4AauMMre42",
    "outputId": "418cae44-01da-4c63-efb5-91d07f611aaa"
   },
   "outputs": [],
   "source": [
    "# Mounting Google drive\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "lMnMggEQf98E"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Importing required libaries\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# Importing required libaries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_selection import VarianceThreshold, RFE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, roc_auc_score, f1_score, plot_confusion_matrix, plot_roc_curve, classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JjJcBGl5f98I"
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZg7tEIRf98J"
   },
   "source": [
    "# Reading and understanding the data¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "id": "_FUMuX9Sf98L",
    "outputId": "82a22b85-5372-48bc-9577-a871081248f9"
   },
   "outputs": [],
   "source": [
    "# Loading data\n",
    "data0= pd.read_csv('telecom_churn_data.csv')\n",
    "data0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uv14tdRWf98N",
    "outputId": "b58b5aca-ecf1-42a3-cc14-37f6b6c700f3"
   },
   "outputs": [],
   "source": [
    "# Checking shape of dataframe\n",
    "data0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o6yLQcYcf98N",
    "outputId": "2df0126b-68df-4a91-83dd-4c3c899f6abe"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data0' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Checking dataframe info\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mdata0\u001b[49m\u001b[38;5;241m.\u001b[39minfo(verbose\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data0' is not defined"
     ]
    }
   ],
   "source": [
    "#Checking dataframe info\n",
    "data0.info(verbose= 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "g-QGBPUvf98O",
    "outputId": "412c1334-0687-4afa-94e0-cfd161f8f0fb"
   },
   "outputs": [],
   "source": [
    "#Checking descriptive statistics\n",
    "data0.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EFp-tiqXf98Q"
   },
   "source": [
    "### Explaining dataset\n",
    "\n",
    "After eyeballing and exploring the data, we can categorize 226 different attributes into below categories:\n",
    "\n",
    "- mobile_number\n",
    "- circle_id\n",
    "- aon : days on network\n",
    "- 31*4= 124 mou (Minutes of Usage) columns for 4 months and 3 consolidated columns (loc_og_t2o_mou, std_og_t2o_mou, loc_ic_t2o_mou)\n",
    "- 12 date columns related to different last recharges for 4 different months\n",
    "- 12 arpu (Average Revenue Per User) columns across different segments for 4 different months\n",
    "- 36 columns contains information about different recharges done by the user for each of these 4 months: total_rech_num_* (no. of total recharges), total_rech_amt_* (amount of total recharge), total_rech_data_* (no. of total data recharges (2g + 3g), max_rech_data_* (maximum amount of data recharge), av_rech_amt_data_* (average amount of each data recharge), count_rech_2g/3g_* (count of 2g and 3g recharges in each month), last_day_rch_amt_* (last recharge amount by the user)\n",
    "- 8 vol_* columns have information of consumed 2g and 3g data volume by the user for each 4 months.\n",
    "- 4 *_vbc_3g contains info about volume based 3g consumption by the users.\n",
    "- 4 night_pck_user_* categorical features contain if a user has active night pack for a month.\n",
    "- 16 sachet_* and monthly_* columns have information about the no. of monthly/sachet packs used by users.\n",
    "- 4 fb_user_* categorical features contain if a user has fb service activated or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_a_2Aczaf98j"
   },
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dFoEpEbOf98l",
    "outputId": "82847310-d6fa-444f-b7f9-92155c2d7564"
   },
   "outputs": [],
   "source": [
    "# Checking for duplicate entry\n",
    "data0['mobile_number'].duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "31sAZcqff98n",
    "outputId": "122571ff-b771-42a6-bf5a-b1aba6fa5efe",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking % of missing values\n",
    "round(data0.isnull().sum()/ data0.shape[0], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQFOjl0Jf98o"
   },
   "source": [
    "There are 39 columns having missing data > 70%. We'll analyze thes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bN5euE9hf98p"
   },
   "source": [
    "## Missing Value Analysis (MCAR, MAR, MNAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnzX8hoDf98p"
   },
   "source": [
    "We'll perform missing value analysis to understand if observed missing values in different columns are MCAR, MAR or MNAR. **We'll start with the assumption that missing values in diffrent columns are MAR** and will try to establish the assumption. At this stage we'll only impute missing values with business knowledge only. We'll not perform any kind of statistical imputation at this stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRY_ODUvf98p"
   },
   "source": [
    "### Analysing missing values in date_of_last_rech_* and total_rech_data_*\n",
    "\n",
    "**Assumption:** We can assume that Null values in date_of_last_rech_* columns are denoting that the customer has not recharged in that month. Then for those datapoints total_rech_data_*, av_rech_amt_data_*, max_rech_data_* columns also should have Null values. If these 4 types of columns have null values for exact indexes, then we can consider our assumption as True and can impute missing values in total_rech_data_*, av_rech_amt_data_*, max_rech_data_* columns with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9k31yBOPf98q",
    "outputId": "e1199a49-466e-445c-f05f-3c0c7ab80528"
   },
   "outputs": [],
   "source": [
    "# Comparing index of missing values across all 4 column types\n",
    "\n",
    "check1= ((data0[data0.date_of_last_rech_data_6.isnull()].index == data0[data0.total_rech_data_6.isnull()].index).all()) & \\\n",
    "((data0[data0.date_of_last_rech_data_6.isnull()].index == data0[data0.av_rech_amt_data_6.isnull()].index).all()) & \\\n",
    "((data0[data0.date_of_last_rech_data_6.isnull()].index == data0[data0.max_rech_data_6.isnull()].index).all())\n",
    "\n",
    "check2= ((data0[data0.date_of_last_rech_data_7.isnull()].index == data0[data0.total_rech_data_7.isnull()].index).all()) & \\\n",
    "((data0[data0.date_of_last_rech_data_7.isnull()].index == data0[data0.av_rech_amt_data_7.isnull()].index).all()) & \\\n",
    "((data0[data0.date_of_last_rech_data_7.isnull()].index == data0[data0.max_rech_data_7.isnull()].index).all())\n",
    "\n",
    "check3= ((data0[data0.date_of_last_rech_data_8.isnull()].index == data0[data0.total_rech_data_8.isnull()].index).all()) & \\\n",
    "((data0[data0.date_of_last_rech_data_8.isnull()].index == data0[data0.av_rech_amt_data_8.isnull()].index).all()) & \\\n",
    "((data0[data0.date_of_last_rech_data_8.isnull()].index == data0[data0.max_rech_data_8.isnull()].index).all())\n",
    "\n",
    "check4= ((data0[data0.date_of_last_rech_data_9.isnull()].index == data0[data0.total_rech_data_9.isnull()].index).all()) & \\\n",
    "((data0[data0.date_of_last_rech_data_9.isnull()].index == data0[data0.av_rech_amt_data_9.isnull()].index).all()) & \\\n",
    "((data0[data0.date_of_last_rech_data_9.isnull()].index == data0[data0.max_rech_data_9.isnull()].index).all())\n",
    "\n",
    "if check1 & check2 & check3 & check4:\n",
    "    print('Assumption is True, We can impute missing values in total_rech_data_*, av_rech_amt_data_*, max_rech_data_* columns \\\n",
    "          with 0 ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gdh7dshDf98r",
    "outputId": "7248a6c9-6ec8-4931-c34b-674ebaf99c73"
   },
   "outputs": [],
   "source": [
    "# Imputing missing values in total_rech_data_*, av_rech_amt_data_*, max_rech_data_* columns\n",
    "\n",
    "cols1= ['total_rech_data_6', 'av_rech_amt_data_6', 'max_rech_data_6', \n",
    "                    'total_rech_data_7', 'av_rech_amt_data_7', 'max_rech_data_7', \n",
    "                    'total_rech_data_8', 'av_rech_amt_data_8', 'max_rech_data_8',\n",
    "                    'total_rech_data_9', 'av_rech_amt_data_9', 'max_rech_data_9']\n",
    "\n",
    "for col in cols1:\n",
    "    data0[col].fillna(0, inplace= True)\n",
    "\n",
    "\n",
    "data0[cols1].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bue9wMkyf98s"
   },
   "source": [
    "### Analysing missing values in arpu_2g_*, arpu_3g_*, count_rech_2g_*, count_rech_3g_*  and night_pck_user_*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CbxnlC33f98s",
    "outputId": "35786f07-0a98-431b-ee30-391383691386"
   },
   "outputs": [],
   "source": [
    "# Checking value_counts\n",
    "data0.night_pck_user_6.value_counts(dropna= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5TEo7Dyf98t"
   },
   "source": [
    "**Assumption**: We'll again check if arpu_2g_*, arpu_3g_*, count_rech_2g_*, count_rech_3g_*  and night_pck_user_* columns have missing values only for those observations for which date_of_last_rech_data_* for that corresponsing month is also missing. If above statement is True, then we can impute these missing values with 0. Again for night_pck_user_ it can be seen to prominent categories 0 : Customers not having night packs, 1: customers having night packs. NaN may signify that the customer is not making any call during night, means an Inactive customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MZQBSZ3if98t",
    "outputId": "17622e3b-f25e-48c9-ab09-1fced38bb294"
   },
   "outputs": [],
   "source": [
    "# Comparing index of missing values across all 5 column types\n",
    "\n",
    "check1= ((data0[data0.date_of_last_rech_data_6.isnull()].index == data0[data0.arpu_2g_6.isnull()].index).all()) & \\\n",
    "((data0[data0.date_of_last_rech_data_6.isnull()].index == data0[data0.arpu_3g_6.isnull()].index).all()) & \\\n",
    "((data0[data0.date_of_last_rech_data_6.isnull()].index == data0[data0.night_pck_user_6.isnull()].index).all()) & \\\n",
    "((data0[data0.date_of_last_rech_data_6.isnull()].index == data0[data0.count_rech_2g_6.isnull()].index).all()) & \\\n",
    "((data0[data0.date_of_last_rech_data_6.isnull()].index == data0[data0.count_rech_3g_6.isnull()].index).all())\n",
    "\n",
    "check2= ((data0[data0.date_of_last_rech_data_7.isnull()].index == data0[data0.arpu_2g_7.isnull()].index).all()) & \\\n",
    "((data0[data0.date_of_last_rech_data_7.isnull()].index == data0[data0.arpu_3g_7.isnull()].index).all()) & \\\n",
    "((data0[data0.date_of_last_rech_data_7.isnull()].index == data0[data0.night_pck_user_7.isnull()].index).all()) & \\\n",
    "((data0[data0.date_of_last_rech_data_7.isnull()].index == data0[data0.count_rech_2g_7.isnull()].index).all()) & \\\n",
    "((data0[data0.date_of_last_rech_data_7.isnull()].index == data0[data0.count_rech_3g_7.isnull()].index).all())\n",
    "\n",
    "check3= ((data0[data0.date_of_last_rech_data_8.isnull()].index == data0[data0.arpu_2g_8.isnull()].index).all()) & \\\n",
    "((data0[data0.date_of_last_rech_data_8.isnull()].index == data0[data0.arpu_3g_8.isnull()].index).all()) & \\\n",
    "((data0[data0.date_of_last_rech_data_8.isnull()].index == data0[data0.night_pck_user_8.isnull()].index).all()) & \\\n",
    "((data0[data0.date_of_last_rech_data_8.isnull()].index == data0[data0.count_rech_2g_8.isnull()].index).all()) & \\\n",
    "((data0[data0.date_of_last_rech_data_8.isnull()].index == data0[data0.count_rech_3g_8.isnull()].index).all())\n",
    "\n",
    "check4= ((data0[data0.date_of_last_rech_data_9.isnull()].index == data0[data0.arpu_2g_9.isnull()].index).all()) & \\\n",
    "((data0[data0.date_of_last_rech_data_9.isnull()].index == data0[data0.arpu_3g_9.isnull()].index).all()) & \\\n",
    "((data0[data0.date_of_last_rech_data_9.isnull()].index == data0[data0.night_pck_user_9.isnull()].index).all()) & \\\n",
    "((data0[data0.date_of_last_rech_data_9.isnull()].index == data0[data0.count_rech_2g_9.isnull()].index).all()) & \\\n",
    "((data0[data0.date_of_last_rech_data_9.isnull()].index == data0[data0.count_rech_3g_9.isnull()].index).all())\n",
    "\n",
    "\n",
    "if check1 & check2 & check3 & check4:\n",
    "    print('Assumption is True, We can impute missing values in arpu_*, night_pck_user_* columns with 0 ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HeoCkNRVf98u",
    "outputId": "72564456-3434-4a15-c9e2-a04a8fbdea2a"
   },
   "outputs": [],
   "source": [
    "# Imputing missing values in arpu_2g_*, arpu_3g_*, night_pck_user_* columns\n",
    "\n",
    "cols2= ['arpu_2g_6', 'arpu_2g_7', 'arpu_2g_8', 'arpu_2g_9', 'arpu_3g_6', 'arpu_3g_7', 'arpu_3g_8', 'arpu_3g_9',\n",
    "       'night_pck_user_6', 'night_pck_user_7', 'night_pck_user_8', 'night_pck_user_9', 'count_rech_2g_6', 'count_rech_3g_6',\n",
    "       'count_rech_2g_7', 'count_rech_3g_7', 'count_rech_2g_8', 'count_rech_3g_8', 'count_rech_2g_9', 'count_rech_3g_9',]\n",
    "\n",
    "\n",
    "for col in cols2:\n",
    "    data0[col].fillna(0, inplace= True)\n",
    "\n",
    "\n",
    "data0[cols2].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UOf120Zvf98v"
   },
   "source": [
    "### Analysing missing values fb_user_* columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JbxUC4KTf98w",
    "outputId": "033c95d0-92f7-4bca-a72c-d7c820c879b6"
   },
   "outputs": [],
   "source": [
    "# Checking value_counts()\n",
    "data0.fb_user_6.value_counts(dropna= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nkEIPz2Nf98w"
   },
   "source": [
    "**Assumption**: Checking if fb_user_* columns have missing values only for those observations for which date_of_last_rech_data_* for that corresponsing month is missing. If above statement is True, then we can impute these missing values with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W375-6oef98x",
    "outputId": "24d39ff9-5986-4b3c-d506-efd25b8709cc"
   },
   "outputs": [],
   "source": [
    "# Comparing index of missing values for fb_user_* columns\n",
    "\n",
    "check1= ((data0[data0.date_of_last_rech_data_6.isnull()].index == data0[data0.fb_user_6.isnull()].index).all())\n",
    "check2= ((data0[data0.date_of_last_rech_data_7.isnull()].index == data0[data0.fb_user_7.isnull()].index).all())\n",
    "check3= ((data0[data0.date_of_last_rech_data_8.isnull()].index == data0[data0.fb_user_8.isnull()].index).all())\n",
    "check4= ((data0[data0.date_of_last_rech_data_9.isnull()].index == data0[data0.fb_user_9.isnull()].index).all())\n",
    "\n",
    "if check1 & check2 & check3 & check4:\n",
    "    print('Assumption is True, We can impute missing values in fb_user_* columns with 0 ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u0foGzTjf98z",
    "outputId": "6793aa96-e1fd-4f81-f9fc-63e54327f072"
   },
   "outputs": [],
   "source": [
    "# Imputing missing values in total_rech_data_, av_rech_amt_data_, max_rech_data_* columns\n",
    "\n",
    "cols3= ['fb_user_6', 'fb_user_7', 'fb_user_8', 'fb_user_9']\n",
    "\n",
    "\n",
    "for col in cols3:\n",
    "    data0[col].fillna(0, inplace= True)\n",
    "\n",
    "\n",
    "data0[cols3].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p5wKKUcBf980",
    "outputId": "26abcf92-9fb2-4b6f-98e8-b51026a8a380"
   },
   "outputs": [],
   "source": [
    "# Checking columns having missing values \n",
    "round(data0.isnull().sum()/ data0.shape[0], 2).sort_values(ascending= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pL9XJ1_Rf981"
   },
   "source": [
    "There are 29 x 4= 116 incoming and outgoing call minutes of usage columns (*mou_\\* columns, og_others\\* and ic_others_\\*) where we can see missing values are present. It's very tiresome activity to tally indexes of all these columns to confirm our assumption that these missing values are MAR. We'll use missingno package to see the correlation heatmap of missing values for these columns one by one for each month. If we get correlation of 1, then we can say these are MAR, these missing values have an observed relation with missing values in other columns. Then we will **compare any one of these columns with total_og_mou_\\* (total outgoing minutes in a moth) and total_ic_mou_\\* (total incoming minutes in a month).** If observations with missing values in this one representative column also have 0 value in both total_og_mou and total_ic_mou columns for the corresponding month then we can conclude that: As total incoming and outgoing mou is zero in that month, so any sub-class values for that month will also be 0. So, if assumption is True then we can impute missing values in all these columns with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 832
    },
    "id": "mLLMw_84f981",
    "outputId": "d3794893-798f-48f7-9af9-0ca9a1428417"
   },
   "outputs": [],
   "source": [
    "# Checking correlation of missing values between all *_mou6 columns\n",
    "msno.heatmap(data0.loc[:, data0.columns.str.endswith(('mou_6', '_others_6', 'total_og_mou_6'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 832
    },
    "id": "yAYaqU5Xf982",
    "outputId": "244ee051-e820-40d6-c787-89cb8af5cffe"
   },
   "outputs": [],
   "source": [
    "# Checking correlation of missing values between all *_mo7 columns\n",
    "msno.heatmap(data0.loc[:, data0.columns.str.endswith(('mou_7', '_others_7'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 832
    },
    "id": "_qsSWvosf982",
    "outputId": "e1ba91f4-0774-40ad-e871-4f8c35d47795"
   },
   "outputs": [],
   "source": [
    "# Checking correlation of missing values between all *_mou8 columns\n",
    "msno.heatmap(data0.loc[:, data0.columns.str.endswith(('mou_8', '_others_8'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 832
    },
    "id": "Lyl0wN0ff983",
    "outputId": "0faeaf5a-c68b-4c28-beb7-ad11d7f4b936"
   },
   "outputs": [],
   "source": [
    "# Checking correlation of missing values between all *_mou9 columns\n",
    "msno.heatmap(data0.loc[:, data0.columns.str.endswith(('mou_9', '_others_9'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yc7njYxgf984",
    "outputId": "913d3b9b-7288-4ab5-87c5-5f0ad8ce8509"
   },
   "outputs": [],
   "source": [
    "# Getting indexes of observations for which onnet_mou_* is missing for month 6,7,8,9\n",
    "ind6= data0[data0.onnet_mou_6.isna()].index\n",
    "ind7= data0[data0.onnet_mou_7.isna()].index\n",
    "ind8= data0[data0.onnet_mou_8.isna()].index\n",
    "ind9= data0[data0.onnet_mou_9.isna()].index\n",
    "\n",
    "# Checking values of total incoming and total outgoing mou for all 4 months for observations having above indexes.\n",
    "\n",
    "print('Month 6 incoming calls for observations having missing mou data: ', *data0.loc[ind6, 'total_ic_mou_6'].unique())\n",
    "print('Month 6 outgoing calls for observations having missing mou data: ', *data0.loc[ind6, 'total_og_mou_6'].unique())\n",
    "print('Month 7 incoming calls for observations having missing mou data: ', *data0.loc[ind7, 'total_ic_mou_7'].unique())\n",
    "print('Month 7 outgoing calls for observations having missing mou data: ', *data0.loc[ind7, 'total_og_mou_7'].unique())\n",
    "print('Month 8 incoming calls for observations having missing mou data: ', *data0.loc[ind8, 'total_ic_mou_8'].unique())\n",
    "print('Month 8 outgoing calls for observations having missing mou data: ', *data0.loc[ind8, 'total_og_mou_8'].unique())\n",
    "print('Month 9 incoming calls for observations having missing mou data: ', *data0.loc[ind9, 'total_ic_mou_9'].unique())\n",
    "print('Month 9 outgoing calls for observations having missing mou data: ', *data0.loc[ind9, 'total_og_mou_9'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09e_EVPhf985"
   },
   "source": [
    "Above, we can see all these observations have 0 values. So, our assumption is True and we can impute missing values in all above columns with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WVKynRNMf985",
    "outputId": "4758233b-a93f-4ec0-980a-b3ea89a55311"
   },
   "outputs": [],
   "source": [
    "# Missing value imputation\n",
    "mou_cols= data0.loc[:,data0.columns.str.endswith(('mou_6', 'mou_7', 'mou_8', '_others_6', '_others_7','_others_8', 'mou_9', '_others_9'))].columns\n",
    "for col in mou_cols:\n",
    "    data0[col].fillna(0, inplace= True)\n",
    "\n",
    "# Checking missing values again\n",
    "data0.isna().sum().sort_values(ascending= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_WigEE1f986"
   },
   "source": [
    "## Dropping unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VmEMfh_Af986",
    "outputId": "1c581d21-c20c-4625-fe57-8ecaf3974832"
   },
   "outputs": [],
   "source": [
    "# Checking value_counts for loc_og_t2o_mou , std_og_t2o_mou , loc_ic_t2o_mou columns\n",
    "print(data0.loc_og_t2o_mou.value_counts(dropna= False))\n",
    "print(data0.std_og_t2o_mou.value_counts(dropna= False))\n",
    "print(data0.loc_ic_t2o_mou.value_counts(dropna= False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dci9NZ1Jf987"
   },
   "source": [
    "All these columns have 0 value and missing values. As it's minutes of usage column it can not be categorical. Even if we impute these missing values using mean, median imputation value will be 0. That will make these columns zero variance column with mean 0. Information Value for these columns will be 0. hence dropping these columns instead of imputing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2MnKKD-if987"
   },
   "outputs": [],
   "source": [
    "# Dropping above 3 columns\n",
    "data0.drop(['loc_og_t2o_mou', 'std_og_t2o_mou', 'loc_ic_t2o_mou'], axis= 1, inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkunhYmif988"
   },
   "source": [
    "Droping all date columns, as we have redundant information from other features so these date columns will not add much of new insights for our analysis. Also dropping mobile_number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KRm-6vzBf99D"
   },
   "outputs": [],
   "source": [
    "# Dropping mobile no. column\n",
    "data0.drop('mobile_number', axis= 1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F1995seif99D",
    "outputId": "5d141e72-ed4b-4633-c887-db443739d27d"
   },
   "outputs": [],
   "source": [
    "# Dropping all date columns\n",
    "print('Shape before dropping:', data0.shape)\n",
    "data0= data0.loc[:, ~data0.columns.str.contains('date_of')]\n",
    "print('Shape after dropping:', data0.shape)\n",
    "\n",
    "# Checking missing values again\n",
    "data0.isna().sum().sort_values(ascending= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5yhQxzHf99E"
   },
   "source": [
    "All columns have now 0 missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gAMR3Lb7f99E"
   },
   "source": [
    "## Renaming columns and changing data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mq1KNZKTf99F",
    "outputId": "2eb041a7-bf8e-49a6-d69f-209282acaf8c"
   },
   "outputs": [],
   "source": [
    "# Columns not having _<month no.> as suffix\n",
    "data0.loc[:,~ data0.columns.str.endswith(('_6','_7','_8', '_9'))].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UEsmi1VPf99F"
   },
   "outputs": [],
   "source": [
    "# Renaming the columns\n",
    "data0.rename(columns= {'jun_vbc_3g': 'vbc_3g_6', 'jul_vbc_3g': 'vbc_3g_7', 'aug_vbc_3g':'vbc_3g_8', 'sep_vbc_3g':'vbc_3g_9'}, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P1eWuIg4s7jk"
   },
   "outputs": [],
   "source": [
    "# Changing to integer from float\n",
    "cat_col= ['fb_user_6', 'night_pck_user_6', 'fb_user_7', 'night_pck_user_7', 'fb_user_8', 'night_pck_user_8']\n",
    "data0[cat_col]= data0[cat_col].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9HcQtONOf99G"
   },
   "outputs": [],
   "source": [
    "data1= data0.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LyqycbMFf99G"
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3rBGOy72f99G"
   },
   "source": [
    "To identify high-value customers we'll perform feature engineering. We'll calculate average amount of recharge done by customers in Good Months (6, 7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K-pFvYVgf99H",
    "outputId": "cce12a03-6a52-4170-cf7e-b4db763d0696"
   },
   "outputs": [],
   "source": [
    "# Calculating total recharge amount for data in each month and dropping original columns\n",
    "\n",
    "data1['total_rech_amt_data_6']= data1.total_rech_data_6 * data1.av_rech_amt_data_6\n",
    "data1['total_rech_amt_data_7']= data1.total_rech_data_7 * data1.av_rech_amt_data_7\n",
    "data1['total_rech_amt_data_8']= data1.total_rech_data_8 * data1.av_rech_amt_data_8\n",
    "data1['total_rech_amt_data_9']= data1.total_rech_data_9 * data1.av_rech_amt_data_9\n",
    "\n",
    "print('Shape before dropping:', data1.shape)\n",
    "\n",
    "data1.drop(['total_rech_data_6', 'av_rech_amt_data_6', 'total_rech_data_7', 'av_rech_amt_data_7',\n",
    "           'total_rech_data_8', 'av_rech_amt_data_8', 'total_rech_data_9', 'av_rech_amt_data_9'], axis= 1, inplace= True)\n",
    "\n",
    "print('Shape after dropping:', data1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-9SWzSTdf99H",
    "outputId": "9fb50d9f-f134-4817-cde2-b0efb2ce4a72"
   },
   "outputs": [],
   "source": [
    "# Adding ARPU of data (3g and 2g) and dropping original columns\n",
    "\n",
    "data1['arpu_data_6']=  data1.arpu_2g_6 + data1.arpu_3g_6\n",
    "data1['arpu_data_7']=  data1.arpu_2g_7 + data1.arpu_3g_7\n",
    "data1['arpu_data_8']=  data1.arpu_2g_8 + data1.arpu_3g_8\n",
    "data1['arpu_data_9']=  data1.arpu_2g_9 + data1.arpu_3g_9\n",
    "\n",
    "print('Shape before dropping:', data1.shape)\n",
    "data1.drop(['arpu_2g_6', 'arpu_3g_6', 'arpu_2g_7', 'arpu_3g_7',\n",
    "           'arpu_2g_8', 'arpu_3g_8', 'arpu_2g_9', 'arpu_3g_9'], axis= 1, inplace= True)\n",
    "print('Shape after dropping:', data1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yqT0V_nZf99I"
   },
   "outputs": [],
   "source": [
    "# Calculating average recharge amount for month 6 and 7 combined\n",
    "data1['avg_rech_amt_6_7']= (data1.total_rech_amt_data_6 + data1.total_rech_amt_data_7 + \n",
    "                            data1.total_rech_amt_6 + data1.total_rech_amt_7)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RlG554hGf99I"
   },
   "source": [
    "## Identifying high-value customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uY25dNDzf99P",
    "outputId": "0d8ef07a-9315-419c-fdf9-6a25edb205b6"
   },
   "outputs": [],
   "source": [
    "# Taking top 70 percentile customers as High Value customers\n",
    "data_hvc= data1[data1.avg_rech_amt_6_7 > np.percentile(data1['avg_rech_amt_6_7'], 70)]\n",
    "\n",
    "# Checking shape\n",
    "data_hvc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IjCzYowhf99Q"
   },
   "source": [
    "## Tagging churners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_MaRTHJf99R"
   },
   "source": [
    "We'll use total_ic_mou_9, total_og_mou_9, vol_2g_mb_9, vol_3g_mb_9 columns to tag the curners. For churners there will not be any voice and data usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XSUFu0CAf99R",
    "outputId": "0c9d03a9-a8e4-4ab9-ee78-00ab7a256049"
   },
   "outputs": [],
   "source": [
    "# Creating churn column and updating value of churn with 1 for the customers having no voice /data usage in month 9\n",
    "data_hvc['churn']= 0\n",
    "data_hvc.loc[(data_hvc.total_ic_mou_9== 0) & (data_hvc.total_og_mou_9== 0) & (data_hvc.vol_2g_mb_9== 0) & (data_hvc.vol_3g_mb_9== 0), 'churn']= 1\n",
    "data_hvc.churn.value_counts(dropna= True, normalize= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EM_IeYLVf99S"
   },
   "outputs": [],
   "source": [
    "# Channging data type of churn column\n",
    "data_hvc['churn']= data_hvc.churn.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iCYzsFnBf99S",
    "outputId": "92c85314-4cd8-4d10-bba8-11ef1951f834"
   },
   "outputs": [],
   "source": [
    "# Now droping columns belong to month 9\n",
    "col_9= data_hvc.loc[:, data_hvc.columns.str.endswith('_9')].columns\n",
    "print('Shape before dropping:', data_hvc.shape)\n",
    "data_hvc.drop(col_9, axis= 1, inplace= True)\n",
    "print('Shape after dropping:', data_hvc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHCA4RtUf99T"
   },
   "source": [
    "## Dropping columns having zero variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y_Uve8yDf99T",
    "outputId": "919c90ef-821d-4661-d198-db6eaa1fe76e"
   },
   "outputs": [],
   "source": [
    "### Droping Columns having zero variance\n",
    "var_t= VarianceThreshold(threshold= 0)\n",
    "variance_thresh= var_t.fit(data_hvc)\n",
    "col_ind= var_t.get_support()\n",
    "\n",
    "# Below data_hvc have zero variance\n",
    "data_hvc.loc[:, ~col_ind].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oBEBcQsdf99U",
    "outputId": "c24b5003-21f7-4f0e-f023-9ff6965aa8a1"
   },
   "outputs": [],
   "source": [
    "# Dropping columns\n",
    "data_hvc.drop(data_hvc.loc[:, ~col_ind].columns, axis= 1, inplace= True)\n",
    "data_hvc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nstw0KYRf99b"
   },
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "id": "jpjo15AEf99d",
    "outputId": "cc9e0433-7171-40cb-8469-772c00c78639"
   },
   "outputs": [],
   "source": [
    "# Checking churn data\n",
    "plt.figure(figsize= [7,5])\n",
    "sns.countplot(data_hvc.churn, palette= 'Paired', label=[1,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZTpkzeJf99d"
   },
   "source": [
    "We have already derived few fetaures and based on domain knowledge we have identified important featured to perform further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LvX0Wezuf99e"
   },
   "outputs": [],
   "source": [
    "# columns to analyze\n",
    "num_columns_to_analyze= ['total_rech_amt_data_6', 'arpu_data_6', 'arpu_6', 'onnet_mou_6', 'offnet_mou_6', 'total_og_mou_6', \n",
    "                         'total_ic_mou_6', 'vol_2g_mb_6', 'vol_3g_mb_6','total_rech_amt_data_7', 'arpu_data_7', 'arpu_7', \n",
    "                         'onnet_mou_7', 'offnet_mou_7', 'total_og_mou_7', 'total_ic_mou_7', 'vol_2g_mb_7', 'vol_3g_mb_7', \n",
    "                         'total_rech_amt_data_8', 'arpu_data_8', 'arpu_8', 'onnet_mou_8', 'offnet_mou_8', 'total_og_mou_8', \n",
    "                         'total_ic_mou_8', 'vol_2g_mb_8', 'vol_3g_mb_8','aon'\n",
    "                        ]\n",
    "\n",
    "char_columns_to_analyze= ['fb_user_6', 'night_pck_user_6', 'fb_user_7', 'night_pck_user_7', 'fb_user_8', 'night_pck_user_8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i4xP8aIQf99e",
    "outputId": "6bdbaf39-5827-4b87-d45c-5312ee28657e"
   },
   "outputs": [],
   "source": [
    "# Dividing the data into two dataframes\n",
    "data_hvc_0= data_hvc[data_hvc.churn== 0]\n",
    "print('Shape of data_hvc_0:', data_hvc_0.shape)\n",
    "data_hvc_1= data_hvc[data_hvc.churn== 1]\n",
    "print('Shape of data_hvc_1:', data_hvc_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CdUZRIOwf99e"
   },
   "outputs": [],
   "source": [
    "# Function for univariate analysis of categorical variables\n",
    "def cat_univariate(app_df_new_0, app_df_new_1, col, fn_sup= 14, fn_s= 12, figsize= [20, 7], xtick_ro= 0):\n",
    "    t0_col = float(len(app_df_new_0))\n",
    "    t1_col = float(len(app_df_new_1))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    fig= plt.figure(figsize= figsize)\n",
    "    f1_x_label= f'{col} for churn= 0'\n",
    "    f2_x_label= f'{col} for churn= 1'\n",
    "    ax1= fig.add_subplot(1,2,1)\n",
    "    ax1.set_xticklabels(f1_x_label, rotation= xtick_ro, ha= 'right',  fontdict= {'fontsize': fn_s, 'color': 'Teal'})\n",
    "    ax2= fig.add_subplot(1,2,2)\n",
    "    ax2.set_xticklabels(f2_x_label, rotation= xtick_ro, ha= 'right',  fontdict= {'fontsize': fn_s, 'color': 'Teal'})\n",
    "    sup_t= f'Count plot for {col}'\n",
    "    fig.suptitle(sup_t, fontdict= {'fontsize': fn_sup, 'color': 'Teal'})\n",
    "    fig1= sns.countplot(data= app_df_new_0, x= col, ax= ax1, palette= 'Paired')\n",
    "    fig2= sns.countplot(data= app_df_new_1, x= col, ax= ax2, palette= 'Paired_r')\n",
    "    fig1.set_ylabel('Count', fontdict= {'fontsize': fn_s, 'color': 'Black'})\n",
    "    fig1.set_xlabel(f1_x_label, fontdict= {'fontsize': fn_s, 'color': 'Black'})\n",
    "    fig2.set_xlabel(f2_x_label, fontdict= {'fontsize': fn_s, 'color': 'Black'})\n",
    "    for patch in fig1.patches:\n",
    "        percentage = '{:.1f}%'.format(100 * patch.get_height()/t0_col)\n",
    "        x= patch.get_x() + patch.get_width()\n",
    "        y= patch.get_height()\n",
    "        fig1.annotate(percentage, (patch.get_x() + patch.get_width() / 2.,\n",
    "                patch.get_height()), ha= 'center', va= 'center', \n",
    "                xytext= (0, 5), textcoords= 'offset points', fontsize= 11, family= 'verdana')\n",
    "        \n",
    "    for patch2 in fig2.patches:\n",
    "        percentage= '{:.1f}%'.format(100 * patch2.get_height()/t1_col)\n",
    "        x= patch2.get_x() + patch2.get_width()\n",
    "        y= patch2.get_height()\n",
    "        fig2.annotate(percentage, (patch2.get_x() + patch2.get_width() / 2.,\n",
    "                patch2.get_height()), ha= 'center', va= 'center', \n",
    "                xytext = (0, 5), textcoords= 'offset points', fontsize= 11, family='verdana')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "tbIRy6WUf99f",
    "outputId": "b585054f-2f77-4414-ee56-588a69ebd672"
   },
   "outputs": [],
   "source": [
    "# Checking character columns\n",
    "for col in char_columns_to_analyze:\n",
    "    cat_univariate(app_df_new_0= data_hvc_0, app_df_new_1= data_hvc_1, col= col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "trFz8W1Rf99g"
   },
   "outputs": [],
   "source": [
    "# Function to plot numeric columns distribution\n",
    "def num_univariate(app_df_new_0, app_df_new_1, col, fn_sup= 14, fn_s= 12, figsize=[18, 7], xtick_ro= 0):\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    fig= plt.figure(figsize= figsize)\n",
    "    sup_t= f'Density plot for {col}'\n",
    "    fig.suptitle(sup_t, fontdict= {'fontsize': fn_sup, 'color': 'Teal'})\n",
    "    sns.distplot(app_df_new_0[app_df_new_0[col].notna()][col], hist= False, label= 'Non-churn')\n",
    "    sns.distplot(app_df_new_1[app_df_new_1[col].notna()][col], hist= False, label='Churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Fsw_JKqwf99g",
    "outputId": "c5fc02c8-9009-4b58-b90e-4d46f36a5ebd"
   },
   "outputs": [],
   "source": [
    "# Distplot of numeric columns\n",
    "for col in num_columns_to_analyze:\n",
    "    num_univariate(app_df_new_0= data_hvc_0, app_df_new_1= data_hvc_1, col= col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ryKyGMSpf99h",
    "outputId": "0ba3fcd1-909d-4be3-da58-65f28129b7c7"
   },
   "outputs": [],
   "source": [
    "# Let's see the correlation matrix \n",
    "plt.figure(figsize = (20,20))\n",
    "sns.heatmap(data_hvc.drop('churn', axis=1).corr(), cmap= 'coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xDhJZD8rf99i",
    "outputId": "aefe49d7-9218-4024-cf8a-9c44ebd3d76f"
   },
   "outputs": [],
   "source": [
    "# Finding top 100 High correlated features\n",
    "a= data_hvc.corr()\n",
    "corr_0= a.where(np.triu(np.ones(a.shape), k=1).astype(np.bool))\n",
    "corr_0= corr_0.unstack().dropna()\n",
    "corr_0= pd.DataFrame(corr_0).reset_index()\n",
    "corr_0.columns= ['Var 1','Var 2','correlation']\n",
    "corr_0['abs_correlation']= np.abs(corr_0['correlation'])\n",
    "corr_0.sort_values('abs_correlation', ascending= False).head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5oulDD9Wf99j"
   },
   "source": [
    "## Inference:\n",
    "- We can see high class imbalance in data.\n",
    "- If we compare month 6, 7, 8, percentage of fb_user is gradually reducing for churn users. Users who are likely to churn gradually stopped using this service. Again, more than 50% of non-churn users use this service, where for churn users it's always below 50% and in Action month (August) it's only 14%. Users quitting this service\n",
    "- Reduction of night pack users can be seen within the churn users. In June 1.6% of churned users used to use night pack which is almost similar of whole population. But in the month of July, it became .9% for the churned users and in Action month (August) only .4% of the churned users were using the night pack, which is considerably smaller than the whole population. So, users who suddenly stop using night packs are likely to churn.\n",
    "- Most of the numeric features are right skewed. We'll take care of this during scaling by performing Robust Scaling, using median and quantile values.\n",
    "- Most of the features have high correlation. As, first we want to build an interpretable model, we can't perform PCA as it'll change the actual features and Principal Components will not have any business interpretation.\n",
    "\n",
    "**1st Approach**: We'll use RFE to reduce correlated features and then we'll build Logistic Regression model and will check VIF and p-value simultaneously to remove multicollinearity and to find statistically significant beta coefficients for identified features.\n",
    "\n",
    "**2nd Approach:** Then we'll try PCA and will explore Blackbox models to achieve better performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RF-zQqBSf99j"
   },
   "outputs": [],
   "source": [
    "# Copying data for data preparation\n",
    "data_hvc1= data_hvc.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqgbjOqLf99k"
   },
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htnmyqE-f99k"
   },
   "source": [
    "## Train-Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FKNYQ_pcf99k"
   },
   "outputs": [],
   "source": [
    "# Train-Test Split\n",
    "y= data_hvc1['churn']\n",
    "X= data_hvc1.drop('churn', axis= 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, train_size= .7, stratify= y, random_state= 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9RtWxy_Nf99l"
   },
   "source": [
    "## Oversampling of minority class using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XZ8lrPkmf99l",
    "outputId": "91d6b2ba-5c3f-4202-89ed-d92ca4237104"
   },
   "outputs": [],
   "source": [
    "### Oversampling minority class with SMOTE\n",
    "smote= SMOTE(random_state= 42)\n",
    "print('Values before oversampling:\\n', y_train.value_counts())\n",
    "X_train_sm, y_train_sm= smote.fit_resample(X_train, y_train)\n",
    "X_train_sm= pd.DataFrame(X_train_sm, columns= X_train.columns)\n",
    "print('Values after oversampling:\\n', pd.DataFrame(y_train_sm).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "doeyycTFf99m"
   },
   "source": [
    "## Scaling numeric features\n",
    "\n",
    "During EDA we have observed few outliers in numeric features. So, using Robust Scaling using median and quantile values instead of Standard Scaling using mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6VN_kwthf99m",
    "outputId": "6bbed058-04cd-4322-ed9b-3f9bbc1da9e1"
   },
   "outputs": [],
   "source": [
    "# Selecting columns for scaling\n",
    "all_cols= X_train_sm.columns.tolist()\n",
    "print('All columns: ', len(all_cols))\n",
    "columns_to_remove= ['fb_user_6', 'night_pck_user_6', 'fb_user_7', 'night_pck_user_7', 'fb_user_8', 'night_pck_user_8']\n",
    "for col in columns_to_remove:\n",
    "    all_cols.remove(col)\n",
    "print('All columns after removing: ', len(all_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ekI4nZ_vf99m"
   },
   "outputs": [],
   "source": [
    "# Performing Robust Scaling\n",
    "scaler= RobustScaler(quantile_range=(2, 98))\n",
    "scaler.fit(X_train_sm[all_cols]) # Fitting on Training dataset\n",
    "X_train_sm[all_cols]= scaler.transform(X_train_sm[all_cols]) # Transforming training dataset\n",
    "X_test[all_cols]= scaler.transform(X_test[all_cols]) # Transforming testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 240
    },
    "id": "kq2UJF7Sf99n",
    "outputId": "7edd95d0-b761-4f1a-ccf2-41e1f64f365e"
   },
   "outputs": [],
   "source": [
    "# Checking scaled features and shape of training data\n",
    "print(X_train_sm.shape)\n",
    "X_train_sm[all_cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CuADcaOZf99n"
   },
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c319a3GEf99o"
   },
   "source": [
    "We'll first build Logistic Regression model. Then We'll also explore different Blackbox models to improve overall model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1Urx1lxf99o"
   },
   "source": [
    "## 1. Logistic Regression (RFE + Manual tunning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxYvqj66f99p"
   },
   "source": [
    "### Using RFE to select top 20 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kiXGIFLwf99p",
    "outputId": "6489438a-49ca-4498-ad5f-97a21c81d37a"
   },
   "outputs": [],
   "source": [
    "# Selecting to 20 features for Logistic Regression using RFE\n",
    "estimator= LogisticRegression(max_iter= 1000, random_state= 42)\n",
    "selector= RFE(estimator, n_features_to_select= 20)\n",
    "selector= selector.fit(X_train_sm, y_train_sm)\n",
    "selected_cols= X_train_sm.loc[:,selector.support_].columns\n",
    "selected_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p4BORJd8f99p"
   },
   "outputs": [],
   "source": [
    "# Selecting top 20 features\n",
    "X_train_final= X_train_sm[selected_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqxuu3CAf99q"
   },
   "source": [
    "### Building 1st Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2pKEKczIf99q"
   },
   "outputs": [],
   "source": [
    "# Building Logistic Regression model using statsmodels\n",
    "\n",
    "X_train_final= sm.add_constant(X_train_final) # Adding constraint\n",
    "lreg1= sm.GLM(y_train_sm, X_train_final, family= sm.families.Binomial())\n",
    "lreg_model_1= lreg1.fit() # Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qAfwPnLJf99q",
    "outputId": "d2c66d09-ef4d-466d-afc7-a142b5833b1e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking the summary of Logistic Regression model\n",
    "print(lreg_model_1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rxAzgCapf99r"
   },
   "outputs": [],
   "source": [
    "# Creating function to calculate VIFs\n",
    "\n",
    "def vif_calculation(X_df):\n",
    "    vif= pd.DataFrame()\n",
    "    X= X_df.drop('const', axis= 1)\n",
    "    vif['Features'] = X.columns\n",
    "    vif['VIF']= [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    vif['VIF']= round(vif['VIF'], 2)\n",
    "    return (vif.sort_values('VIF', ascending= False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 665
    },
    "id": "1iauWgnUNrE_",
    "outputId": "3d4fa378-c226-4e33-b0c2-d3035be33c86"
   },
   "outputs": [],
   "source": [
    "# Calculating the VIFs for the 1st model\n",
    "vif_calculation(X_train_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-QVTjneOYV1"
   },
   "source": [
    "All coefficients have p values less than .05 but arpu_7 and total_rech_amt_7 have high VIF. Average Revenue per user (arpu) should have direct correlation with total_rech_amt for the same month. Let's remove total_rech_amt_7 and building the model again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6IZKiP1OYau"
   },
   "source": [
    "### Building 2nd Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Uw6TZtuNr13"
   },
   "outputs": [],
   "source": [
    "# Building Logistic Regression model using statsmodels\n",
    "X_train_final.drop('total_rech_amt_7', axis= 1, inplace= True) # Removing column\n",
    "lreg2= sm.GLM(y_train_sm, X_train_final, family= sm.families.Binomial())\n",
    "lreg_model_2= lreg2.fit() # Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VsCKNeWANtO0",
    "outputId": "3e3946ee-db25-4992-bfdc-daa8b6992b0d"
   },
   "outputs": [],
   "source": [
    "# Checking the summary of Logistic Regression model\n",
    "print(lreg_model_2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 634
    },
    "id": "m6nqRXKFNtI_",
    "outputId": "3914d98f-b560-46d4-f0cc-bab1c523dcbf"
   },
   "outputs": [],
   "source": [
    "# Calculating the VIFs for the 2nd model\n",
    "vif_calculation(X_train_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZLm6RY4O9j4"
   },
   "source": [
    "All beta coefficients have p values less than .05 except arpu_7, removing it and checking again in next model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXk0_Gm3O_As"
   },
   "source": [
    "### Building 3rd Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7mBr86U6NtBu"
   },
   "outputs": [],
   "source": [
    "# Building Logistic Regression model using statsmodels\n",
    "X_train_final.drop('arpu_7', axis= 1, inplace= True) # Removing column\n",
    "lreg3= sm.GLM(y_train_sm, X_train_final, family= sm.families.Binomial())\n",
    "lreg_model_3= lreg3.fit() # Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ctaND2ANs5a",
    "outputId": "c08286a7-af7c-468c-8624-ae9c3e2f22b9"
   },
   "outputs": [],
   "source": [
    "# Checking the summary of Regression model\n",
    "print(lreg_model_3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 603
    },
    "id": "ZQy6KxHpNswP",
    "outputId": "dccea9d0-0001-4be2-a222-3f53ac4c8ce2"
   },
   "outputs": [],
   "source": [
    "# Calculating the VIFs for the 3rd model\n",
    "vif_calculation(X_train_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5QEA4aFkPHaB"
   },
   "source": [
    "All beta coefficients have p values less than .05 but loc_ic_mou_8 has high VIF, removing it and checking again in next model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LyDnTehPHXI"
   },
   "source": [
    "### Building 4th Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bLLNNp93Nsje"
   },
   "outputs": [],
   "source": [
    "# Building Logistic Regression model using statsmodels\n",
    "X_train_final.drop('loc_ic_mou_8', axis= 1, inplace= True) # Removing column\n",
    "lreg4= sm.GLM(y_train_sm, X_train_final, family= sm.families.Binomial())\n",
    "lreg_model_4= lreg4.fit() # Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pCRo2DHlNsQe",
    "outputId": "118adf58-766a-414e-9d55-49a1db14ea9f"
   },
   "outputs": [],
   "source": [
    "# Checking the summary of Logistic Regression model\n",
    "print(lreg_model_4.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "KelbcuxWVsj5",
    "outputId": "84e69560-bee4-4681-c33a-a4792e30f589"
   },
   "outputs": [],
   "source": [
    "# Calculating the VIFs for the 4th model\n",
    "vif_calculation(X_train_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3APUYhOPZWG"
   },
   "source": [
    "All beta coefficients have p values less than .05 but loc_ic_mou_7 has high VIF, removing it and checking again in next model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mz2c7bUJWGld"
   },
   "source": [
    "### Building 5th Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FM-CQbVNWHBJ"
   },
   "outputs": [],
   "source": [
    "# Building Logistic Regression model using statsmodels\n",
    "X_train_final.drop('loc_ic_mou_7', axis= 1, inplace= True) # Removing column\n",
    "lreg5= sm.GLM(y_train_sm, X_train_final, family= sm.families.Binomial())\n",
    "lreg_model_5= lreg5.fit() # Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c-WH7Qc5WHEI",
    "outputId": "ebccb368-1447-4590-a128-fef5bbd249da"
   },
   "outputs": [],
   "source": [
    "# Checking the summary of Logistic Regression model\n",
    "print(lreg_model_5.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "6LfCrSRYWHNn",
    "outputId": "49e3b13b-f689-4e7d-8c71-aae4ae5f373c"
   },
   "outputs": [],
   "source": [
    "# Calculating the VIFs for the 5th model\n",
    "vif_calculation(X_train_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VTXIWKhEWfcz"
   },
   "source": [
    "All beta coefficients have p values less than .05 and VIF values are also lower than 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V9KV6AGsPdAR",
    "outputId": "cba6de7c-c181-430a-802a-457cc55fe403"
   },
   "outputs": [],
   "source": [
    "# Evaluating on Training dataset\n",
    "y_train_pred= pd.DataFrame(lreg_model_5.predict(X_train_final), columns=['prob'])\n",
    "y_train_pred['pred_churn']= y_train_pred.prob.map(lambda x: 1 if x > 0.5 else 0) # Setting decision margin at .5\n",
    "y_train_pred= y_train_pred.merge(pd.DataFrame(y_train_sm, columns= ['churn']), how= 'inner', left_index= True, right_index= True)\n",
    "# Get Confusion matrix\n",
    "tn,fp,fn,tp= confusion_matrix(y_true= y_train_pred.churn, y_pred= y_train_pred.pred_churn).ravel()\n",
    "print('Confusion Matrix on Training dataset:')\n",
    "print('True Negative:',tn, '    ','False Positive:',fp)\n",
    "print('False Negative:',fn,'    ','True Positive:',tp, '\\n')\n",
    "print('Classification Report on Training dataset:\\n', classification_report(y_true= y_train_pred.churn, y_pred= y_train_pred.pred_churn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5LwHgz0QPfaZ"
   },
   "outputs": [],
   "source": [
    "# Craeting a function to plot ROC curve\n",
    "\n",
    "def roc_plot(actual, probs):\n",
    "    fpr, tpr, thresholds= roc_curve(actual, probs, drop_intermediate = False )\n",
    "    auc_score= roc_auc_score(actual, probs)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "    plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "pPrNo-jIPgTu",
    "outputId": "844397d4-d2bb-4c36-c775-d004c002c130"
   },
   "outputs": [],
   "source": [
    "# Ploting ROC curve\n",
    "fpr, tpr, thresholds= roc_curve(y_train_pred.churn, y_train_pred.pred_churn, drop_intermediate = False )\n",
    "roc_plot(y_train_pred.churn, y_train_pred.prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_337RonPZfY"
   },
   "source": [
    "### Finding Optimal Probability Cutoff Point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "-n9CFoQWPaBR",
    "outputId": "9f3ffa24-3021-4112-d1da-664f79c77844"
   },
   "outputs": [],
   "source": [
    "# Creating different label columns using different probability cutoffs\n",
    "num= [float(x)/10 for x in range(10)]\n",
    "for i in num:\n",
    "    y_train_pred[i]=  y_train_pred.prob.map(lambda x: 1 if x > i else 0)\n",
    "y_train_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "jNj3zicrPaE1",
    "outputId": "59bf3f5f-2dd2-4c3f-ce22-73f44e24aa19"
   },
   "outputs": [],
   "source": [
    "# Calculating accuracy sensitivity and specificity for various probability cutoffs.\n",
    "\n",
    "plot_df= pd.DataFrame(columns = ['prob','accuracy','sensitivity','specificity'])\n",
    "\n",
    "for n in num:\n",
    "    TN,FP,FN,TP= confusion_matrix(y_true= y_train_pred.churn, y_pred= y_train_pred[n]).ravel()\n",
    "    accuracy= (TN+TP)/float(TN+FP+FN+TP)\n",
    "    specificity= TN / float(TN+FP)\n",
    "    sensitivity= TP / float(TP+FN)\n",
    "    plot_df.loc[n]= [n,accuracy,sensitivity,specificity]\n",
    "    \n",
    "plot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "Pc5QTYNFPaIK",
    "outputId": "ba893133-15f6-4446-ee9c-5e888407e187"
   },
   "outputs": [],
   "source": [
    "# Ploting Accuracy, Sensitivity and Specificity for different probability cutoffs\n",
    "\n",
    "plot_df.plot.line(x= 'prob', y= ['accuracy','sensitivity','specificity'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYVjzftaPzHl"
   },
   "source": [
    "Probability cutoff .5 is well balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VsOLc7IAP1AV"
   },
   "source": [
    "### Final Logistic Regression Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NiQ-5gjJPaMB",
    "outputId": "3bd06c0c-fac8-4087-c987-dd33f1568b78"
   },
   "outputs": [],
   "source": [
    "print(lreg_model_5.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J2DulLBWPaO9",
    "outputId": "e3ae9abc-7eeb-433f-8c18-361748bc5ed0"
   },
   "outputs": [],
   "source": [
    "# Evaluating on Testing dataset\n",
    "final_cols= X_train_final.columns.tolist()\n",
    "final_cols.remove('const')\n",
    "X_test_final= X_test[final_cols]\n",
    "X_test_final= sm.add_constant(X_test_final) # Adding constraints\n",
    "y_test_pred= pd.DataFrame(lreg_model_5.predict(X_test_final), columns=['prob'])\n",
    "y_test_pred['pred_churn']= y_test_pred.prob.map(lambda x: 1 if x > 0.5 else 0) # Setting decision margin at .5\n",
    "y_test_pred= y_test_pred.merge(pd.DataFrame(y_test, columns= ['churn']), how= 'inner', left_index= True, right_index= True)\n",
    "lr_final_cl_report_test= classification_report(y_true= y_test_pred.churn, y_pred= y_test_pred.pred_churn)\n",
    "print('Classification Report on Testing Dataset:\\n', lr_final_cl_report_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4z_GDQ99P9eo"
   },
   "source": [
    "- We have got almost similar Accuracy and Recall on training and testing dataset.\n",
    "- On training dataset, we have got Recall of .81 and overall accuracy .81\n",
    "- According to business requirements we need to identify potential users who are likely to churn, we need a model with good Recall/Sensitivity (The model should identify as many True Positive as possible, in this process the model may identify some False Positives as probable churn customer).\n",
    "- Here probability cutoff .5 is well balanced. If business wants to increase Recall/Sensitivity further, then this probability cut-off can be reduced to .4 or .3, but in that case specificity and precision will reduce further (More users will be identified as churners who are not).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKA9OxuMf99x"
   },
   "source": [
    "## Performing PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "id": "O1Ash8Nff99x",
    "outputId": "8af6c91f-eaf0-4b1f-e3a0-4251d2a353de",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Performing PCA and scree plot\n",
    "pca= PCA(random_state= 42)\n",
    "pca.fit(X_train_sm)\n",
    "plt.figure(figsize= [12,7])\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_)+1), np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('No. of Principal Components')\n",
    "plt.ylabel('Cumulative variance explained')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "id": "6UUypVfEf99y",
    "outputId": "42e49eb5-ed34-418a-a5ba-ccf2d9a769dc"
   },
   "outputs": [],
   "source": [
    "# Checking no. of components required to explain 97% variance\n",
    "pca= PCA(.97, random_state= 42)\n",
    "pca.fit(X_train_sm)\n",
    "plt.figure(figsize= [12,7])\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_)+1), np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('No. of Principal Components')\n",
    "plt.ylabel('Cumulative variance explained')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6OpCoAgyf99y"
   },
   "source": [
    "39 principal components can explain 97% variance in traininging dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9E_kVt05f99z"
   },
   "outputs": [],
   "source": [
    "# Transforming training and tetsing dataset\n",
    "X_train_pc= pca.transform(X_train_sm)\n",
    "X_test_pc= pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G9m4sMNlf99z",
    "outputId": "5f03b11e-a78a-42ea-dc2a-57494f728334"
   },
   "outputs": [],
   "source": [
    "# Shape of new test and train dataset\n",
    "X_train_pc.shape, X_test_pc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 432
    },
    "id": "QrKdtFIkf990",
    "outputId": "d103638d-706c-4781-9554-957f47c2044e"
   },
   "outputs": [],
   "source": [
    "# Checking correlation coefficients of principal components\n",
    "plt.figure(figsize= (12,7))\n",
    "sns.heatmap(pd.DataFrame(X_train_pc).corr(), cmap= 'coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSnSb7MAf990"
   },
   "source": [
    "As all principal components are orthogonal to each other, so they have no correlation with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TdkMp0X-f991"
   },
   "source": [
    "## 2. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y19jzNatf991"
   },
   "outputs": [],
   "source": [
    "# Using Random Forest Classifier\n",
    "rfc= RandomForestClassifier(random_state= 42)\n",
    "rfc.fit(X_train_pc, y_train_sm)\n",
    "y_train_pred= rfc.predict(X_train_pc)\n",
    "y_test_pred= rfc.predict(X_test_pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3vjVReuEz_1u",
    "outputId": "78b52d0a-c089-4f01-971e-c09a6be48359"
   },
   "outputs": [],
   "source": [
    "# Classification report on training dataset\n",
    "print(classification_report(y_train_sm, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XPPHhvANyS7G",
    "outputId": "53415fbe-83a0-45d4-e12e-fa29af626e4e"
   },
   "outputs": [],
   "source": [
    "# Classification report on testing dataset\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4ceH6aif992"
   },
   "source": [
    "It can be seen that our Random Forest model is showing overfitting. We'll use hyperparameter tunning to reduce the variance of the model. According to our business requirement we need higher recall/sensitivity for class 1. That means the model should identify as much True Positive as possible (users likely to churn, should not be missed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BHKcelvBf992"
   },
   "source": [
    "### Random Forest- Hyperparameter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K8CO-3dtf992",
    "outputId": "05eb5c59-4c53-4e26-d2b5-ccbba11f989e"
   },
   "outputs": [],
   "source": [
    "# Performing GridSearchCV\n",
    "rfc= RandomForestClassifier(random_state= 42)\n",
    "param_grid= { 'n_estimators': [100, 200, 300, 500],\n",
    "             'max_depth': [4, 5, 9, 12, 15],\n",
    "             'min_samples_leaf': [15, 20, 25]\n",
    "             }\n",
    "gcv_rfc= GridSearchCV(estimator= rfc, param_grid= param_grid, cv= 3, scoring= 'recall', n_jobs= -1, return_train_score= True, verbose= 1)\n",
    "gcv_rfc_fit= gcv_rfc.fit(X_train_pc, y_train_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CfzUfpKuf993",
    "outputId": "b02eb6f8-d684-4b66-b660-d8c577a11a75"
   },
   "outputs": [],
   "source": [
    "# Checking best parameters\n",
    "gcv_rfc_fit.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "G9cqy3-Uf993",
    "outputId": "5b998989-4c2f-4ae2-98c1-bd06ee137e98"
   },
   "outputs": [],
   "source": [
    "# Storing CV result\n",
    "rfcv_df= pd.DataFrame(gcv_rfc_fit.cv_results_)\n",
    "#joblib.dump(rfcv_df, '/content/drive/MyDrive/colab_data/rfcv_df.pkl')\n",
    "#rfcv_df= joblib.load('/content/drive/MyDrive/colab_data/rfcv_df.pkl')\n",
    "rfcv_df.sort_values('mean_train_score', ascending= False).head(100) # Displaying top 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "77jURCluf993",
    "outputId": "53f57aec-a22c-4afd-bcc2-102b8b6e07a7"
   },
   "outputs": [],
   "source": [
    "# Plotting param_max_depth vs mean_train_score (mean of cross validation accuracy)\n",
    "plt.figure(figsize= (12,7))\n",
    "sns.lineplot(data= rfcv_df, x= 'param_max_depth', y= 'mean_train_score' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BWMUUq1otlH"
   },
   "source": [
    "We can see that with increase in max_depth, recall is incraesing. After 12 slope has been reduced. We'll go with 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "NXI84AflmwsC",
    "outputId": "afce7635-deef-47aa-d6ef-417551e9b84b"
   },
   "outputs": [],
   "source": [
    "# Plotting param_max_depth vs mean_train_score (mean of cross validation recall) for param_max_depth= 12\n",
    "plt.figure(figsize= (12,7))\n",
    "sns.lineplot(data= rfcv_df[(rfcv_df.param_max_depth == 12)], x= 'param_n_estimators', y= 'mean_train_score' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJ3U-gYOpGbi"
   },
   "source": [
    "It can be seen with max_depth= 12, after n_estimators= 300 there is almost no change in score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "CA9NbxaVn4U3",
    "outputId": "1907061f-c298-4490-a01f-aae5ca7f93c8"
   },
   "outputs": [],
   "source": [
    "# Plotting param_max_depth vs mean_train_score (mean of cross validation accuracy) for param_max_depth= 15\n",
    "plt.figure(figsize= (12,7))\n",
    "sns.lineplot(data= rfcv_df[(rfcv_df.param_max_depth == 12) & (rfcv_df.param_n_estimators== 300)], x= 'param_min_samples_leaf', y= 'mean_train_score' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u3JJdLXYrRY5",
    "outputId": "d17d4a6d-392c-491e-ab6f-472bd5834460"
   },
   "outputs": [],
   "source": [
    "# Building Random Forest model with above parameters\n",
    "rfc= RandomForestClassifier(n_estimators= 300, max_depth= 12, min_samples_leaf= 15, random_state= 42)\n",
    "rfc.fit(X_train_pc, y_train_sm)\n",
    "y_train_pred= rfc.predict(X_train_pc)\n",
    "y_test_pred= rfc.predict(X_test_pc)\n",
    "print('Accuracy on trining data:' ,accuracy_score(y_train_sm, y_train_pred))\n",
    "print('Accuracy on testing data:' ,accuracy_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ydKRJeaJFPve",
    "outputId": "0f423345-a83f-48a2-a205-add52a50d554"
   },
   "outputs": [],
   "source": [
    "# Classification report on training dataset\n",
    "print('Accuracy on testing data: \\n' ,classification_report(y_train_sm, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Crx3yupprwiF",
    "outputId": "f9c8f9ab-cabb-4f11-875d-ef21ab3c0795"
   },
   "outputs": [],
   "source": [
    "# Classification report on testing dataset\n",
    "rfc_final_cl_report_test= classification_report(y_test, y_test_pred)\n",
    "print('Accuracy on testing data: \\n' ,rfc_final_cl_report_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLqKHRjgM9Iw"
   },
   "source": [
    "O finally, after hyperparameters tunning we have got overall testing accuracy of .88 and recall of .72."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3BgzEtcFLvL8"
   },
   "source": [
    "## XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h_1oM8lPCo5b",
    "outputId": "96c26785-1183-4164-ca5d-b8746a4dcaee"
   },
   "outputs": [],
   "source": [
    "# Using XGBoost Classifier\n",
    "xgbcl= XGBClassifier(random_state= 42)\n",
    "xgbcl.fit(X_train_pc, y_train_sm)\n",
    "y_train_pred= xgbcl.predict(X_train_pc)\n",
    "y_test_pred= xgbcl.predict(X_test_pc)\n",
    "print('Accuracy on trining data:' ,accuracy_score(y_train_sm, y_train_pred))\n",
    "print('Accuracy on testing data:' ,accuracy_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yWuE4j0kMFcv",
    "outputId": "5814d58c-e29b-40f7-ad10-1e5f104e1e85"
   },
   "outputs": [],
   "source": [
    "# Classification report on training dataset\n",
    "print('Accuracy on testing data: \\n' ,classification_report(y_train_sm, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fwGBcIhYMIYL",
    "outputId": "e6f102dd-6384-4089-b16f-a9c130bd36fe"
   },
   "outputs": [],
   "source": [
    "# Classification report on testing dataset\n",
    "print('Accuracy on testing data: \\n' ,classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idHzYGLvM-s0"
   },
   "source": [
    "### XGBoost Classifier - Hyperparameter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ez3vaGwSNFhg"
   },
   "outputs": [],
   "source": [
    "# Param grid\n",
    "param_grid= {'n_estimators': [100, 200, 300, 500],\n",
    "        'gamma': [.5, .7, 1],\n",
    "        'subsample': [.6,.9, 1],\n",
    "        'colsample_bytree': [.6, .9, 1],\n",
    "        'max_depth': [4, 6, 8, 9],\n",
    "        'learning_rate': [.01, .05, .1, .5]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ho9plI7N0zB",
    "outputId": "b295c369-1761-4e83-ba10-9181fd54ab24"
   },
   "outputs": [],
   "source": [
    "# Performing GridSearchCV\n",
    "xgbcl= XGBClassifier(tree_method='gpu_hist', predictor='gpu_predictor', random_state= 42)\n",
    "gcv_xgbcl= GridSearchCV(estimator= xgbcl, param_grid= param_grid, cv= 3, verbose=3, n_jobs= -1, scoring= 'recall', return_train_score= True)\n",
    "gcv_xgbcl_fit= gcv_xgbcl.fit(X_train_pc, y_train_sm)\n",
    "\n",
    "# Storing CV result\n",
    "xgbcl_df= pd.DataFrame(gcv_xgbcl_fit.cv_results_)\n",
    "#joblib.dump(xgbcl_df, '/content/drive/MyDrive/colab_data/xgbcl_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dS8JzIxbDW9H"
   },
   "outputs": [],
   "source": [
    "# Loading stores result\n",
    "#xgbcl_df= joblib.load('/content/drive/MyDrive/colab_data/xgbcl_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4VLTFHbOOW-c",
    "outputId": "c7e88860-5177-4f5a-92b6-b0672ce7e551"
   },
   "outputs": [],
   "source": [
    "# Best estimator\n",
    "gcv_xgbcl_fit.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 773
    },
    "id": "rgNhBNJDDkwC",
    "outputId": "ec4a6835-7f7a-4abc-f756-7417c1e8cabd"
   },
   "outputs": [],
   "source": [
    "# Checking CV result\n",
    "xgbcl_df.sort_values('mean_train_score', ascending= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "50rKEHHiMtp7",
    "outputId": "67593225-5bf9-4d5a-f9f9-93eaf322499b"
   },
   "outputs": [],
   "source": [
    "# Checking best parameter from cv result\n",
    "xgbcl_df.sort_values('mean_train_score', ascending= False).loc[1727, 'params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jwg86NP4Cu4c",
    "outputId": "64124773-d3f4-4b01-95e9-d77c7659d40e"
   },
   "outputs": [],
   "source": [
    "# Using XGBoost Classifier\n",
    "xgbcl= XGBClassifier(booster='gbtree',colsample_bytree= 1, subsample= 1, gamma= 1,  learning_rate= 0.5, max_depth= 9, n_estimators= 500, n_jobs= -1,\n",
    "              random_state=42, verbosity=1)\n",
    "xgbcl.fit(X_train_pc, y_train_sm)\n",
    "y_train_pred= xgbcl.predict(X_train_pc)\n",
    "y_test_pred= xgbcl.predict(X_test_pc)\n",
    "print('Accuracy on trining data:' ,accuracy_score(y_train_sm, y_train_pred))\n",
    "print('Accuracy on testing data:' ,accuracy_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ABfdyjzDDMW",
    "outputId": "454a46c4-5334-45b9-a3d3-704d32956a92"
   },
   "outputs": [],
   "source": [
    "# Classification report on training dataset\n",
    "print('Accuracy on testing data: \\n' ,classification_report(y_train_sm, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7nfb3zfRDHwq",
    "outputId": "7001610f-83ec-4af8-b138-fa28ab59f2e0"
   },
   "outputs": [],
   "source": [
    "# Classification report on testing dataset\n",
    "xgbc_final_cl_report_test= classification_report(y_test, y_test_pred)\n",
    "print('Accuracy on testing data: \\n' ,xgbc_final_cl_report_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OeHL6swmauZn"
   },
   "source": [
    "# Conclusion and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8sfI4aNGXaTc"
   },
   "source": [
    "1. We have performed data preprocessing, Missing Value Analysis, Feature Engineering, identified most valuable customers, tagged churners, and performed required EDA. We have mentioned few inferences observed during EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mzTpgAOdXxEH"
   },
   "source": [
    "2. As part of data preparation, we have split the data into train-test dataset and performed SMOTE on training dataset to handle class imbalance. We have performed scaling (Used Robust scaling to handle outliers) before building our first model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMKnzDMeSXMZ"
   },
   "source": [
    "3. 3.\tWe have used Logistic Regression model on actual features. We have used RFE to select top 20 features and then performed manual tunning to remove multicollinearity and make sure that all beta coefficients are statistically significant. From final Logistic Regression model, we can find below features importance to perform churn prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "3HFgIR7WPWwg",
    "outputId": "f9af7df3-305a-44f9-d68a-688fa50ed99c"
   },
   "outputs": [],
   "source": [
    "# Plotting important features with beta coefficients values\n",
    "plt.figure(figsize= (12,7))\n",
    "lreg_model_5.params.plot(kind= 'barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zOH9n3xTaJXG",
    "outputId": "0d0f0e84-0949-4149-9c19-fd52cf404ba6"
   },
   "outputs": [],
   "source": [
    "# Classifical report of our final logistic regression model\n",
    "print(lr_final_cl_report_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLSrPzvIaKIw"
   },
   "source": [
    "- We have got almost similar Accuracy and Recall on training and testing dataset.\n",
    "- Here probability cutoff .5 is well balanced. If business wants to increase Recall/Sensitivity further, then this probability cut-off can be reduced to .4 or .3, but in that case specificity and precision will reduce further (More users will be identify as churners who are actually not).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3u4XRY7pTsCX"
   },
   "source": [
    "4. Then we have performed PCA and kept 97% of variance by selecting 39 Principal Components. We used Random Forest and then performed hyperparameters tunning to tune our Random Forest model. Final Classification on testing dataset report after performing hyperparameters tunning is as below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MHA6Qd4rirbm",
    "outputId": "47ae99d6-dab7-4406-995e-312803a33586"
   },
   "outputs": [],
   "source": [
    "# Printing classification report\n",
    "print(rfc_final_cl_report_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0XsRSvY8WMyg"
   },
   "source": [
    "5. Followed by that, We used XGBoost Classifier and then perfomed hyperparameters tunning to tune our XGB Classifier model. Final Classification on testing dataset report after performing hyperparameters tunning is as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MgETiYPkPayy",
    "outputId": "a8a251f7-ff5e-43dd-b211-00137de75051"
   },
   "outputs": [],
   "source": [
    "# Printing classification report\n",
    "print(xgbc_final_cl_report_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0ObKWLJjD7H"
   },
   "source": [
    "- If we compare all 3 models. XGBoost has highest accuracy then followed by Random Forest Classifier and Logistic Regression. But according to business requirements to identify potential users who are likely to churn, we need a model with **good Recall/Sensitivity** (The model should identify as many True Positive as possible, in this process the model may identify some False Positives as probable churn customer). In that regards Logistic Regression is the best model in metric of recall.\n",
    "- As discussed earlier, probability cut-off of this logistic regression model can be reduced further to .4, .3 etc. to increase the recall further (if the business wants to be more aggressive to find all potential churn customers).\n",
    "- From beta-coefficients of the Logistic Regression model, it can be seen that:\n",
    " **total_ic_mou_8** (Total incoming call during month August/ action phase) is the most important feature. Other features like **total_rech_num_8** (Total no. of recharge during action phase), **loc_og_mou_8** (Local outgoing call during month 8/ Action phase). It can be seen features of action phase (8) and 2nd month of good phase (7) are the most important to determine if a user is going to churn or not. Business should focus on these features shown in above bar plot to identify possible churners and reach them to understand their pain points. For  example, reduction in total incoming minutes, total local outgoing calls, no. of recharges, 3g data consumption, last recharge amount etc. in action phase (month 8), stopping fb service etc. may indicate high probability of churn.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "tRY_ODUvf98p",
    "Bue9wMkyf98s",
    "RlG554hGf99I"
   ],
   "machine_shape": "hm",
   "name": "telecom_churn_prediction_1.6.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
